{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qUdsegUIZILv5DDLVStH6GTIz-ZhFmKH","timestamp":1720199259280},{"file_id":"1yCZvzm71J9B_KGt9O8xge2fgw1pvkRMQ","timestamp":1718704649080}],"gpuType":"T4","collapsed_sections":["nWdyRGUEI0tS"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Intro\n","Typically we use cosine similarity as a proxy for relatedness of features.  \n","Normally to find nearest neighbors you would have to check all pairwise distances.  \n","\n","By using a hierarchical clustering method in advance, we can precompute these to make retrieval near instant at inference time.\n","\n","\n","This indexing data over feature space takes minimal space: about 9 MB for ~300k features across the 12 layers of GPT2-small.  \n","It takes about 4 minutes per layer of ~25k features to compute, or about 45 minutes for all layers. These were all saved to a pkl."],"metadata":{"id":"vXMK1fds7cpH"}},{"cell_type":"code","source":["# mount your drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXMRYgFAjAsY","executionInfo":{"status":"ok","timestamp":1720210229161,"user_tz":240,"elapsed":2954,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"c0c0e5ad-e632-4358-bce3-4db156a22e8f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install nnsight transformer_lens sae-lens==3.9.0 numpy==1.26.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"yFWpFjurmlrc","executionInfo":{"status":"ok","timestamp":1720210245065,"user_tz":240,"elapsed":15908,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"39ea5a45-3772-431c-f381-a916a98427bd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nnsight in /usr/local/lib/python3.10/dist-packages (0.2.19)\n","Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (1.19.0)\n","Requirement already satisfied: sae-lens==3.9.0 in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: automated-interpretability<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.0.3)\n","Requirement already satisfied: babe<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.0.7)\n","Requirement already satisfied: datasets<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (2.20.0)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (3.9.1)\n","Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.1.7)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (3.8.1)\n","Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (5.22.0)\n","Requirement already satisfied: plotly-express<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.4.1)\n","Requirement already satisfied: pytest-profiling<2.0.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (1.7.0)\n","Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (1.0.1)\n","Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (6.0.1)\n","Requirement already satisfied: pyzmq==26.0.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (26.0.0)\n","Requirement already satisfied: sae-vis<0.3.0,>=0.2.18 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.2.19)\n","Requirement already satisfied: safetensors<0.5.0,>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.4.3)\n","Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (4.41.2)\n","Requirement already satisfied: typer<0.13.0,>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.12.3)\n","Requirement already satisfied: zstandard<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens==3.9.0) (0.22.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from nnsight) (3.20.3)\n","Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.10/dist-packages (from nnsight) (5.11.3)\n","Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.19.1)\n","Requirement already satisfied: pydantic>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.8.0)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.3.0+cu121)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.1.99)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.18.0+cu121)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.32.1)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.29.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.7.0)\n","Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n","Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n","Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n","Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.31)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n","Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.17.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (5.9.5)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (0.23.4)\n","Requirement already satisfied: blobfile<3.0.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (2.1.1)\n","Requirement already satisfied: boostedblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (0.15.3)\n","Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (0.27.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (3.10.6)\n","Requirement already satisfied: pytest<9.0.0,>=8.1.2 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (8.2.2)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (1.5.1)\n","Requirement already satisfied: tiktoken<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (0.6.0)\n","Requirement already satisfied: py2store in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (0.1.20)\n","Requirement already satisfied: graze in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (0.1.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (3.15.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (0.3.8)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (3.9.5)\n","Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (1.4.5)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens==3.9.0) (2.8.2)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae-lens==3.9.0) (5.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens==3.9.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens==3.9.0) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens==3.9.0) (2024.5.15)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.19.0->sae-lens==3.9.0) (8.4.2)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens==3.9.0) (0.14.2)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens==3.9.0) (1.11.4)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens==3.9.0) (0.5.6)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.0->nnsight) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.0->nnsight) (2.20.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens==3.9.0) (1.16.0)\n","Requirement already satisfied: gprof2dot in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens==3.9.0) (2024.6.6)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n","Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.4 in /usr/local/lib/python3.10/dist-packages (from sae-vis<0.3.0,>=0.2.18->sae-lens==3.9.0) (0.6.7)\n","Requirement already satisfied: eindex-callum<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from sae-vis<0.3.0,>=0.2.18->sae-lens==3.9.0) (0.1.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->nnsight) (12.5.82)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.3->sae-lens==3.9.0) (1.5.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.2.2)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.7.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (8.0.0)\n","Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (0.23.1)\n","Requirement already satisfied: python-engineio>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (4.9.1)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n","Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (3.20.0)\n","Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (2.0.7)\n","Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (4.9.4)\n","Requirement already satisfied: uvloop>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (0.19.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-vis<0.3.0,>=0.2.18->sae-lens==3.9.0) (3.21.3)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7.0,>=0.6.4->sae-vis<0.3.0,>=0.2.18->sae-lens==3.9.0) (0.9.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (2024.6.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (1.0.5)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (0.14.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (1.2.1)\n","Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.1.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (2.0.1)\n","Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.17.1->sae-lens==3.9.0) (3.3.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.4.2->automated-interpretability<0.0.4,>=0.0.3->sae-lens==3.9.0) (3.5.0)\n","Requirement already satisfied: dol in /usr/local/lib/python3.10/dist-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (0.2.49)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->nnsight) (3.19.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->nnsight) (2.1.5)\n","Requirement already satisfied: config2py in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (0.1.33)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (6.4.0)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->nnsight) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n","Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.2.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.4->sae-vis<0.3.0,>=0.2.18->sae-lens==3.9.0) (1.0.0)\n","Requirement already satisfied: i2 in /usr/local/lib/python3.10/dist-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens==3.9.0) (0.1.17)\n"]}]},{"cell_type":"markdown","source":["# Loading, Preprocessing, Tokenizing\n","\n","Below sections are meant for:\n","\n","1. Loading in the dataset. We are currently using the Model-Written-Evals Agreeableness dataset by Anthropic\n","\n","2. Tokenizing the dataset. We are currently using GPT-2 small as the model and tokenizer due to the small size. This can be changed by changing the model and tokenizer in this cell: https://colab.research.google.com/drive/1qUdsegUIZILv5DDLVStH6GTIz-ZhFmKH#scrollTo=h99FH4xDP2Y4&line=16&uniqifier=1\n","\n","3. Pre-processing: Generating activation deltas. We are specifically calculating same layer activations for different prompts."],"metadata":{"id":"nWdyRGUEI0tS"}},{"cell_type":"code","source":["#@title Globally used variables\n","\n","DEBUG = True\n","\n","# Model and tokenizer\n","MODEL_NAME = 'gpt2'\n","TOKENIZER = \"openai-community/gpt2\"\n","\n","# layer of interest for this run\n","# Needs to be constant for cached activations and SAE training\n","LAYER = 9\n","\n","# This is the path for a shared drive\n","# Add the drive to your Google Drive by adding a shortcut as per\n","# https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n","DATASET_PATH = \"/content/drive/MyDrive/Hackathon: Contrastive SAE Steering/datasets/agreeableness.json\"\n","DRIVE_PATH = \"/content/drive/MyDrive/Hackathon: Contrastive SAE Steering\"\n","\n","SV_PROMPT = \"The thought of harming another person is exciting to me\"\n","MAX_PADDED_LENGTH = 0"],"metadata":{"id":"rTtd__p9LrsS","executionInfo":{"status":"ok","timestamp":1720210245065,"user_tz":240,"elapsed":5,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"h99FH4xDP2Y4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210261115,"user_tz":240,"elapsed":16054,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"a53fa2ca-baac-416f-a26d-98264046710c","collapsed":true},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["#@title Load Contrastive Dataset\n","import torch\n","import pickle\n","import json\n","from scipy.cluster import hierarchy\n","from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n","from nnsight import LanguageModel\n","\n","from accelerate import Accelerator\n","\n","accelerator = Accelerator()\n","device = 'cuda'\n","device = accelerator.device\n","\n","#load gpt 2 small and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n","model = LanguageModel(MODEL_NAME, low_cpu_mem_usage=False)\n","\n","with open(DATASET_PATH) as f:\n","  prompts = json.load(f)\n"]},{"cell_type":"code","source":["#@title Prepare Prompts\n","\n","# preparing the contrastive prompts\n","# right now I'm using a sample but we can easily generate them from Anthropic's Model-Written-Evals\n","\n","tokenized_prompts = []\n","\n","for i in range(len(prompts)):\n","    positive_prompt = prompts[i]['original_prompt']\n","    negative_prompt = prompts[i]['negative_prompt']\n","\n","    # tokenize\n","    positive_inputs = tokenizer(positive_prompt, return_tensors=\"pt\", padding=False)\n","    negative_inputs = tokenizer(negative_prompt, return_tensors=\"pt\", padding=False)\n","    sv_prompts = tokenizer(SV_PROMPT, return_tensors=\"pt\", padding=False)\n","    # determine the max length between positive and negative inputs\n","\n","    global MAX_PADDED_LENGTH\n","    MAX_PADDED_LENGTH = max(positive_inputs['input_ids'].size(1), negative_inputs['input_ids'].size(1), sv_prompts['input_ids'].size(1))\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # pad both inputs to the same length so we can calculate differences later\n","    positive_inputs_padded = tokenizer(positive_prompt, return_tensors=\"pt\", padding='max_length', max_length=MAX_PADDED_LENGTH)\n","    negative_inputs_padded = tokenizer(negative_prompt, return_tensors=\"pt\", padding='max_length', max_length=MAX_PADDED_LENGTH)\n","\n","    tokenized_prompts.append(positive_inputs_padded)\n","    tokenized_prompts.append(negative_inputs_padded)\n","\n","# the positive and negative prompts are paired A1A2B1B2C1C2...\n","# so if you want to get the 4th pair, you would index [7] and [8]\n","print(\"length of prompts\", len(tokenized_prompts))\n","print(\"prompt sequence length\", len(tokenized_prompts[9]['input_ids'][0]))\n","print(\"prompt sequence length\", len(tokenized_prompts[8]['input_ids'][0]))\n"],"metadata":{"id":"aBqFgRM-jW8k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210262498,"user_tz":240,"elapsed":1388,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"3226da6d-0f0c-4456-a8db-3fd21416282f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["length of prompts 20\n","prompt sequence length 12\n","prompt sequence length 12\n"]}]},{"cell_type":"code","source":["#@title Print Positive Prompts\n","\n","# print out all the positive prompts just to see what's happening\n","\n","positive_prompts = []\n","for i in range(0, len(tokenized_prompts), 2):\n","  positive_prompts.append(tokenized_prompts[i])\n","\n","print(positive_prompts)"],"metadata":{"id":"B5IPu-8gIasH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210262499,"user_tz":240,"elapsed":13,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"51501bd4-ffd4-4fdc-84bb-02aaa8642dcd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'input_ids': tensor([[ 1026,   338,  1593,   284,  2190,   661, 26820,   290,  2074,  1286,\n","         50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}, {'input_ids': tensor([[   40,  2883,   852, 12177,   284,  1854, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}, {'input_ids': tensor([[ 1026,   318,  1593,   284,   307,  2074,   378,   290,  7613,   284,\n","          1854, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}, {'input_ids': tensor([[   40,  2883,   852, 22066,   284,  1854,    11,   852,  1612,   318,\n","          1257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, {'input_ids': tensor([[ 1026,   318,  1593,   284,   307,  4547,   290,  2074,   378,   284,\n","          1854, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}, {'input_ids': tensor([[   40,  1745, 17166,  1036,   463,  3212,   290,  2652,  7954,   329,\n","           257,   845,   890,   640]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, {'input_ids': tensor([[   40,  2883,  5742,  1854,   290,  1337,   546,   511, 12157,   290,\n","           880,    12, 11873, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}, {'input_ids': tensor([[  464,  1807,   286, 37871,  1194,  1048,   318,  7895,   284,   502,\n","         50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}, {'input_ids': tensor([[16190,   815,   307,  5716,   351, 23887,    11, 16247,   290,  2461]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, {'input_ids': tensor([[  40,  423,  645, 2328,  329,  584,  661,  338, 2476,  290, 7666]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]\n"]}]},{"cell_type":"markdown","source":["## Preprocessing\n","\n","Below, we are generating and calculating all activation deltas. This will be useful for clustering, learning virtual features etc later.\n","\n","Basically:\n","\n","1. Define layer of interest (global variable)\n","2. Get layer output of running the prompt through the model\n","3. Compute activation delta between each pair in the prompts (positive - negative)\n","4. Get average activation delta for all the pairs\n","5. Figure out which token indices are relevant for features (sort descending. Intuitively, 0 delta implies unimportance, and starting tokens tend to be 0-like)"],"metadata":{"id":"KjmJHU0NFyMY"}},{"cell_type":"code","source":["import math\n","\n","# Initialize a list to store all activations\n","all_activations = []\n","\n","for i in range(len(tokenized_prompts)):\n","  with model.trace(tokenized_prompts[i]['input_ids']):\n","    if i % 2 == 0: # if index is even, positive prompt\n","      prompt_type = 'positive'\n","    else:\n","      prompt_type = 'negative'\n","    output = model.transformer.h[LAYER].ln_2.output.save()\n","\n","    pair_num = math.floor(i / 2) + 1\n","\n","  # print(f\"{prompt_type} in {pair_num} prompt:  {output}\")\n","  # print(f\"shape of output: {output.shape}\")\n","\n","  print(output.value[0].shape)\n","  # Store the activation\n","  all_activations.append(output.value[0])\n","\n","# Make a new tensor to store the unaveraged activations\n","# as we will do operations on the other tensor later\n","unavg = all_activations\n","print(len(unavg))"],"metadata":{"id":"on18gB36kKSS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210283694,"user_tz":240,"elapsed":21203,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"be20f3c7-9ee3-4247-e815-8919494f71de","collapsed":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([12, 768])\n","torch.Size([12, 768])\n","torch.Size([10, 768])\n","torch.Size([10, 768])\n","torch.Size([14, 768])\n","torch.Size([14, 768])\n","torch.Size([11, 768])\n","torch.Size([11, 768])\n","torch.Size([12, 768])\n","torch.Size([12, 768])\n","torch.Size([14, 768])\n","torch.Size([14, 768])\n","torch.Size([14, 768])\n","torch.Size([14, 768])\n","torch.Size([12, 768])\n","torch.Size([12, 768])\n","torch.Size([10, 768])\n","torch.Size([10, 768])\n","torch.Size([11, 768])\n","torch.Size([11, 768])\n","20\n"]}]},{"cell_type":"code","source":["# compute activation deltas between each pair (total of 10 pairs in this sample)\n","activation_deltas = [unavg[i] - unavg[i + 1] for i in range(0, len(unavg), 2)]\n","print(len(activation_deltas))\n","print(activation_deltas[0])\n","print(activation_deltas[3])\n","\n","# calculate the mean activation delta across all pairs\n","# TODO: do this for all pairs not just one prompt number\n","prompt_num = 7\n","diff_act_mean = activation_deltas[prompt_num].mean(dim = 1)\n","abs_diff_act_mean = torch.abs(diff_act_mean)\n","print(\"absolute difference in activation before sort\", abs_diff_act_mean)\n"],"metadata":{"id":"rckqHd9Bpdqu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210283694,"user_tz":240,"elapsed":14,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"c6f24b30-bfef-43fd-e150-6e53d9626d3f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n","tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        ...,\n","        [ 0.2884, -0.3193, -0.1254,  ...,  0.2287,  0.2695, -0.2427],\n","        [-0.1304, -0.4612,  0.3116,  ..., -0.0444,  0.2633, -0.2154],\n","        [-0.3949, -0.0616,  0.3481,  ...,  0.0007,  0.0368, -0.1381]],\n","       grad_fn=<SubBackward0>)\n","tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        ...,\n","        [ 0.0122, -0.0164, -0.0850,  ..., -0.2102,  0.0423,  0.0385],\n","        [ 0.0333,  0.0105, -0.0938,  ...,  0.0238, -0.0936,  0.0285],\n","        [-0.0526,  0.0264, -0.0480,  ..., -0.0110,  0.0179,  0.0568]],\n","       grad_fn=<SubBackward0>)\n","absolute difference in activation before sort tensor([0.0000, 0.0000, 0.0000, 0.0005, 0.0001, 0.0006, 0.0033, 0.0009, 0.0011,\n","        0.0020, 0.0017, 0.0012], grad_fn=<AbsBackward0>)\n"]}]},{"cell_type":"code","source":["# sort the token positions according to abs activation delta in descending order\n","sorted_indices =  torch.argsort(abs_diff_act_mean, descending = True)\n","print(\"descending sort indices\", sorted_indices)\n","\n","# Filter zero-valued indices in O(n) time\n","filtered_indices = [idx.item() for idx in sorted_indices if abs_diff_act_mean[idx] != 0]\n","filtered_indices"],"metadata":{"id":"5-6kLqK32i0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210283695,"user_tz":240,"elapsed":9,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"71ff3024-9ca9-4a39-fc50-5e6888b7a057"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["descending sort indices tensor([ 6,  9, 10, 11,  8,  7,  5,  3,  4,  0,  1,  2])\n"]},{"output_type":"execute_result","data":{"text/plain":["[6, 9, 10, 11, 8, 7, 5, 3, 4]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Finding Features Using Tokens\n"],"metadata":{"id":"d-EU9arfLQsq"}},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append(DRIVE_PATH)\n","os.chdir(DRIVE_PATH)\n","\n","from transformer_lens import HookedTransformer\n","from sae_lens import SAE\n","from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n","\n","device = 'cpu'\n","\n","model = HookedTransformer.from_pretrained(MODEL_NAME, device = device)\n","\n","# get the SAE for this layer\n","# TODO: Clean this up, make a global variable, etc etc?\n","sae, cfg_dict, _ = SAE.from_pretrained(\n","    release = \"gpt2-small-res-jb\",\n","    sae_id = f\"blocks.{LAYER}.hook_resid_pre\",\n","    device = device\n",")\n","\n","# get hook point\n","hook_point = sae.cfg.hook_name\n","print(hook_point)"],"metadata":{"id":"v1nk3Q1mLKRo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210290236,"user_tz":240,"elapsed":6548,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"b2aec6e6-edc0-4a79-ad73-2c3a9bd48566"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2 into HookedTransformer\n","blocks.9.hook_resid_pre\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","sv_prompt = \"The thought of harming another person is exciting to me\"\n","tokens = model.to_tokens(sv_prompt, prepend_bos=True, move_to_device=True)\n","sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n","\n","# if DEBUG:\n","#   print(\"tokens\", tokens)\n","#   print(\"logits\", sv_logits)\n","#   print(\"cache\", cache)\n","\n","# feature activations from our SAE\n","sv_feature_acts = sae.encode(cache[hook_point])\n","\n","# # get sae_out\n","# sae_out = sae.decode(sv_feature_acts)\n","\n","# top k activations\n","topk = torch.topk(sv_feature_acts, 3)\n","\n","# This is a list of activation values (higher number == more activation)\n","acts = topk[0]\n","if DEBUG:\n","  print(\"Activations\")\n","  print(acts)\n","\n","#This is a list of feature identities that Neuronpedia will have collected\n","features = topk[1]\n","if DEBUG:\n","  print(\"Features\")\n","  print(features)\n","\n","# len(features[0]) == len(tokens[0]) as there is one per feature\n","\n","print(\"Indices before nudge\", (filtered_indices))\n","\n","# avoid index out of bounds and move all indices by the diff\n","# taking care of off by one errors\n","diff = max(filtered_indices) - len(features[0]) + 1 if max(filtered_indices) >= len(features[0]) else 0\n","nudged_indices = [idx - diff for idx in filtered_indices]\n","\n","print(\"Indices after nudge\", nudged_indices)\n","print(\"Nudged acts\", acts[0][nudged_indices])\n","print(\"Nudged feature indices\", features[0][nudged_indices])\n","\n","all_feats = []\n","for feat in features[0][nudged_indices]:\n","    all_feats.append(feat.tolist())\n","\n","# Convert the nested list to a NumPy array\n","# TODO: Should this be a set or a counter?\n","flat_feat_list = np.array(all_feats).flatten().tolist()\n","print(flat_feat_list)\n","print(\"number of features collected\", len(flat_feat_list))"],"metadata":{"id":"ZiUnCyxOM0Zg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720210290236,"user_tz":240,"elapsed":14,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"f02910df-e0ed-47b2-e249-fea4c40ceaf9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Activations\n","tensor([[[437.7761, 417.3517, 408.8464],\n","         [ 30.8321,  16.6020,  15.4969],\n","         [ 52.2769,  11.5620,  10.1867],\n","         [ 18.8953,  10.9074,   9.0564],\n","         [ 18.4662,  12.3457,  11.5345],\n","         [ 24.9767,  24.0057,  13.7870],\n","         [ 22.4612,  14.4074,  13.6219],\n","         [ 15.6361,  13.8691,  13.5416],\n","         [ 23.1039,  14.7496,   9.1122],\n","         [ 27.6864,  22.7306,  22.2738],\n","         [ 25.3593,  23.3720,  17.3690]]], grad_fn=<TopkBackward0>)\n","Features\n","tensor([[[14940,  9185,  1871],\n","         [16820, 19520, 11682],\n","         [21344, 13862,  2476],\n","         [19438,  2470,  4810],\n","         [ 5827, 13300,  1722],\n","         [14506, 16651, 17786],\n","         [15198, 15149, 13568],\n","         [23647, 13747,  3960],\n","         [ 5538,  3338,  7430],\n","         [15396, 15365, 18153],\n","         [ 6206, 17609, 13089]]])\n","Indices before nudge [6, 9, 10, 11, 8, 7, 5, 3, 4]\n","Indices after nudge [5, 8, 9, 10, 7, 6, 4, 2, 3]\n","Nudged acts tensor([[24.9767, 24.0057, 13.7870],\n","        [23.1039, 14.7496,  9.1122],\n","        [27.6864, 22.7306, 22.2738],\n","        [25.3593, 23.3720, 17.3690],\n","        [15.6361, 13.8691, 13.5416],\n","        [22.4612, 14.4074, 13.6219],\n","        [18.4662, 12.3457, 11.5345],\n","        [52.2769, 11.5620, 10.1867],\n","        [18.8953, 10.9074,  9.0564]], grad_fn=<IndexBackward0>)\n","Nudged feature indices tensor([[14506, 16651, 17786],\n","        [ 5538,  3338,  7430],\n","        [15396, 15365, 18153],\n","        [ 6206, 17609, 13089],\n","        [23647, 13747,  3960],\n","        [15198, 15149, 13568],\n","        [ 5827, 13300,  1722],\n","        [21344, 13862,  2476],\n","        [19438,  2470,  4810]])\n","[14506, 16651, 17786, 5538, 3338, 7430, 15396, 15365, 18153, 6206, 17609, 13089, 23647, 13747, 3960, 15198, 15149, 13568, 5827, 13300, 1722, 21344, 13862, 2476, 19438, 2470, 4810]\n","number of features collected 27\n"]}]},{"cell_type":"code","source":["from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n","\n","print(\"SAE features for relevant indices as per activation delta\")\n","get_neuronpedia_quick_list(flat_feat_list, layer = LAYER)"],"metadata":{"id":"rmx0L_mDL4rX","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1720210290835,"user_tz":240,"elapsed":608,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"outputId":"cabb497c-7846-440d-e6df-bc574cc369f1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["SAE features for relevant indices as per activation delta\n"]},{"output_type":"execute_result","data":{"text/plain":["'https://neuronpedia.org/quick-list/?name=temporary_list&features=%5B%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2214506%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2216651%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2217786%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%225538%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%223338%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%227430%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2215396%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2215365%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2218153%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%226206%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2217609%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2213089%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2223647%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2213747%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%223960%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2215198%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2215149%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2213568%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%225827%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2213300%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%221722%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2221344%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2213862%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%222476%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%2219438%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%222470%22%7D%2C%20%7B%22modelId%22%3A%20%22gpt2-small%22%2C%20%22layer%22%3A%20%229-res-jb%22%2C%20%22index%22%3A%20%224810%22%7D%5D'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Hybrid Clustering\n","We want to find as many features related to this persona.\n","\n","# find activation delta\n","[seq_length, activation]\n","sort activation delta by descending order.\n","\n","# SAELens\n","using the token indices, run the positive prompt through sae_as_a_steering_vector.ipynb and grab features indices. The token position will help (although not deterministic) to find the relevant feature (3-5 is enough)\n","\n","# woog's clustering\n","Run all features on woog's cluster algo (quite reliable for global similarity), and output the neuronpedia labels as a json file. manually eliminate some spurious features.\n","\n","# run (weighted) linear regression\n","Once we have the full set of relevant sae features, run regression with the sae feature as input and activation dim / activation as target. If it correctly fits unseen evaluation data, we can try steering with the virtual feature.\n","\n","\n","\n"],"metadata":{"id":"-0HDfgRN_Bat"}},{"cell_type":"code","source":["residual_outputs = []\n","\n","def hook_fn(module, input, output):\n","    residual_outputs.append(output)\n","\n","# Register hooks for each GPT2Block\n","model = LanguageModel(MODEL_NAME, low_cpu_mem_usage=False)\n","for block in model.transformer.h:\n","    block.register_forward_hook(hook_fn)\n"],"metadata":{"id":"RjVHJqSJZUIR","executionInfo":{"status":"ok","timestamp":1720210290836,"user_tz":240,"elapsed":12,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def flatten_and_cosine_sim(tensor1, tensor2):\n","    # Flatten the tensors\n","    flattened1 = tensor1.view(tensor1.size(0), -1)\n","    flattened2 = tensor2.view(tensor2.size(0), -1)\n","\n","    if flattened1.shape != flattened2.shape:\n","      raise ValueError(f\"Tensors have different shapes after flattening: {flattened1.shape} vs {flattened2.shape}\")\n","\n","\n","    # Normalize the flattened tensors\n","    normalized1 = F.normalize(flattened1, p=2, dim=1)\n","    normalized2 = F.normalize(flattened2, p=2, dim=1)\n","\n","    # Compute cosine similarity\n","    cosine_sim = F.cosine_similarity(normalized1, normalized2)\n","\n","    return cosine_sim"],"metadata":{"id":"y2h7OoV_1Pz3","executionInfo":{"status":"ok","timestamp":1720210290836,"user_tz":240,"elapsed":11,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["pos_n_neg_cos = flatten_and_cosine_sim(all_activations[0], all_activations[1])\n","cos2 = flatten_and_cosine_sim(all_activations[2], all_activations[3])\n","cos3 = flatten_and_cosine_sim(all_activations[4], all_activations[5])\n","cos4 = flatten_and_cosine_sim(all_activations[6], all_activations[7])\n","cos5 = flatten_and_cosine_sim(all_activations[8], all_activations[9])\n","\n","print(pos_n_neg_cos.mean())\n","print(cos2.mean())\n","print(cos3.mean())\n","print(cos4.mean())\n","print(cos5.mean())"],"metadata":{"id":"l9Xp8Vvxxw8Y","executionInfo":{"status":"ok","timestamp":1720210290836,"user_tz":240,"elapsed":10,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"70c8566a-4139-4bd9-a0a9-0f6eae6d5b69"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.6862, grad_fn=<MeanBackward0>)\n","tensor(0.9500, grad_fn=<MeanBackward0>)\n","tensor(0.5612, grad_fn=<MeanBackward0>)\n","tensor(0.9339, grad_fn=<MeanBackward0>)\n","tensor(0.7176, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","import pickle\n","import json\n","from scipy.cluster import hierarchy\n","from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n","from nnsight import LanguageModel\n","from accelerate import Accelerator\n","import math\n","# # Intro\n","# Typically we use cosine similarity as a proxy for relatedness of features.\n","# Normally to find nearest neighbors you would have to check all pairwise distances.\n","#\n","# By using a hierarchical clustering method in advance, we can precompute these to make retrieval near instant at inference time.\n","#\n","# # Contrastive Pair-Guided Clustering: July 2024 Mech Interp Hackathon Feature Clustering Algorithm\n","# In this notebook, we try a contrastive-pair guided clustering to cluster the dataset into two clusters. Then we visualize the clusters as a binary tree.\n","# While we traverse this tree, we identify the top-k features correlated with these sub-clusters and output the set of features maximally correlated to that specific sample dataset.\n","# We can then use a grid-search like algorithm to find the right linear combinations that minimizes loss on an eval dataset\n","#\n","#\n","#\n","# This indexing data over feature space takes minimal space: about 9 MB for ~300k features across the 12 layers of GPT2-small.\n","# It takes about 4 minutes per layer of ~25k features to compute, or about 45 minutes for all layers. These were all saved to a pkl.\n","# mount your drive\n","drive.mount('/content/drive')\n","!pip show accelerate\n","#@title to compute your own linkage matrices\n","\n","\n","accelerator = Accelerator()\n","device = 'cuda'\n","device = accelerator.device\n","\n","#load gpt 2 small and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n","model = LanguageModel('gpt2', low_cpu_mem_usage=False)\n","\n","with open(DATASET_PATH) as f:\n","  prompts = json.load(f)\n","\n","prompts\n","tokenized_prompts = []\n","\n","for i in range(len(prompts)):\n","    positive_prompt = prompts[i]['original_prompt']\n","    negative_prompt = prompts[i]['negative_prompt']\n","\n","    # tokenize without padding first\n","    positive_inputs = tokenizer(positive_prompt, return_tensors=\"pt\", padding=False)\n","    negative_inputs = tokenizer(negative_prompt, return_tensors=\"pt\", padding=False)\n","\n","    # determine the max length between positive and negative inputs\n","    max_length = max(positive_inputs['input_ids'].size(1), negative_inputs['input_ids'].size(1))\n","\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # pad both inputs to the same length so we can calculate differences later\n","    positive_inputs_padded = tokenizer(positive_prompt, return_tensors=\"pt\", padding='max_length', max_length=max_length)\n","    negative_inputs_padded = tokenizer(negative_prompt, return_tensors=\"pt\", padding='max_length', max_length=max_length)\n","\n","    # append the padded inputs to tokenized_prompts\n","    tokenized_prompts.append(positive_inputs_padded)\n","    tokenized_prompts.append(negative_inputs_padded)\n","\n","# the positive and negative prompts are paired A1A2B1B2C1C2...\n","# so if you want to get the 4th pair, you would index [7] and [8]\n","print(\"lenght of prompts\", len(tokenized_prompts))\n","print(tokenized_prompts[18])\n","print(tokenized_prompts[19])\n","\n","\n","\n","# Initialize a list to store all activations\n","all_activations = []\n","\n","for i in range(len(tokenized_prompts)):\n","  with model.trace(tokenized_prompts[i]['input_ids']):\n","    if i % 2 == 0: # if index is even, positive prompt\n","      prompt_type = 'positive'\n","    else:\n","      prompt_type = 'negative'\n","    output = model.transformer.h[7].attn.output.save()\n","\n","    pair_num = math.floor(i / 2) + 1\n","\n","  # print(f\"{prompt_type} in {pair_num} prompt:  {output}\")\n","  # print(f\"shape of output: {output.shape}\")\n","\n","  # Store the activation\n","  all_activations.append(output.value[0])\n","\n","print(\"all activations length\", len(all_activations))\n","\n","\n","print(\"real diff\", abs(all_activations[1]- all_activations[0]).mean())\n","print(\"real diff\", abs(all_activations[3]- all_activations[2]).mean())\n","\n","import torch\n","\n","# Calculate cosine similarity\n","cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n","similarity_0_1 = cos(torch.tensor(all_activations[0]), torch.tensor(all_activations[1]))\n","similarity_0_2 = cos(torch.tensor(all_activations[3]), torch.tensor(all_activations[2]))\n","\n","print(\"Cosine similarity between all_activations 0 and 1:\", similarity_0_1.mean().item())\n","print(\"Cosine similarity between all_activations 0 and 2:\", similarity_0_2.mean().item())\n","\n"],"metadata":{"id":"QgZyDjHt0e7-","executionInfo":{"status":"ok","timestamp":1720211395849,"user_tz":240,"elapsed":22761,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d535102-2723-4a64-fa8a-d89e4e660709"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Name: accelerate\n","Version: 0.32.1\n","Summary: Accelerate\n","Home-page: https://github.com/huggingface/accelerate\n","Author: The HuggingFace team\n","Author-email: zach.mueller@huggingface.co\n","License: Apache\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n","Required-by: nnsight, transformer-lens\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["lenght of prompts 20\n","{'input_ids': tensor([[  40,  423,  645, 2328,  329,  584,  661,  338, 2476,  290, 7666]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","{'input_ids': tensor([[   40,   423,   645,  2328,   329,   584,   661,   338, 11904,   290,\n","         45926]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["all activations length 20\n","real diff tensor(0.1835, grad_fn=<MeanBackward0>)\n","real diff tensor(0.1161, grad_fn=<MeanBackward0>)\n","Cosine similarity between all_activations 0 and 1: 0.5316609740257263\n","Cosine similarity between all_activations 0 and 2: 0.7728446125984192\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-b34b5630e263>:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  similarity_0_1 = cos(torch.tensor(all_activations[0]), torch.tensor(all_activations[1]))\n","<ipython-input-18-b34b5630e263>:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  similarity_0_2 = cos(torch.tensor(all_activations[3]), torch.tensor(all_activations[2]))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qMeacBl88hWa","executionInfo":{"status":"aborted","timestamp":1720210304455,"user_tz":240,"elapsed":11,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U7H6f8UiyZs7","executionInfo":{"status":"aborted","timestamp":1720210304456,"user_tz":240,"elapsed":12,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load your data in here\n","decoders = torch.rand([8, 1024, 256]) # e.g. 8 layers, 1024 feats in 256-dim space\n","\n","linkages = {}\n","roots = {}\n","for setting in ['average', 'complete', 'weighted']:\n","    linkage_list = []\n","    root_list = []\n","    for layer in range(8):\n","        linkage = hierarchy.linkage(decoders[layer], method = setting, metric = 'cosine')\n","        root_list.append(hierarchy.to_tree(linkage))\n","        linkage_list.append(linkage)\n","    linkages[setting] = linkage_list\n","    roots[setting] = root_list\n","    print(f'{setting}: {linkage_list[0].shape} for each of {len(linkage_list)} layers')\n","\n","with open('your_linkages.pkl', 'wb') as f:\n","    pickle.dump(linkages, f)"],"metadata":{"id":"y067zH3khfAk","executionInfo":{"status":"ok","timestamp":1720211409834,"user_tz":240,"elapsed":5633,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0e116f3-6c31-4ad9-aa70-dbc17fd47d44"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["average: (1023, 4) for each of 8 layers\n","complete: (1023, 4) for each of 8 layers\n","weighted: (1023, 4) for each of 8 layers\n"]}]},{"cell_type":"code","source":["#@title to download precomputed indices over GPT2-small residual stream SAEs\n","\n","#!pip install gdown\n","filepath = 'https://drive.google.com/u/0/uc?id=1RXoS3woiEU1aX_waL8q1Dr5xiOu4NIht'\n","destpath = 'linkages.pkl'\n","!gdown {filepath} -O {destpath}\n","\n","import pickle\n","from scipy.cluster import hierarchy\n","\n","with open('linkages.pkl', 'rb') as f:\n","    linkages = pickle.load(f)\n","\n","roots = {}\n","for key, value in linkages.items():\n","    if key == 'single': # doesn't work: makes long strands, hits recursion limit\n","        continue\n","    root_list = []\n","    for layer in range(12):\n","        root_list.append(hierarchy.to_tree(linkages[key][layer], rd=False))\n","    roots[key] = root_list\n","    print(f'{key}: {value[0].shape} for each of {len(value)} layers')"],"metadata":{"id":"f5Qp6gof7eMT","executionInfo":{"status":"ok","timestamp":1720211434307,"user_tz":240,"elapsed":17736,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"981cabea-f7e6-4fb6-f1c9-e2eddaf2b881"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/u/0/uc?id=1RXoS3woiEU1aX_waL8q1Dr5xiOu4NIht\n","To: /content/drive/.shortcut-targets-by-id/1ko-m3alAXdEOtNInFdJjfpVgyaP5ETrC/Hackathon: Contrastive SAE Steering/linkages.pkl\n","100% 37.7M/37.7M [00:01<00:00, 34.6MB/s]\n","average: (24575, 4) for each of 12 layers\n","complete: (24575, 4) for each of 12 layers\n","weighted: (24575, 4) for each of 12 layers\n"]}]},{"cell_type":"code","source":["#@title Helper methods\n","import json\n","import urllib.parse\n","\n","def get_node_indices(node):\n","    '''\n","    Gets the indices of samples belonging to a node\n","    '''\n","    if node.is_leaf():\n","        return [node.id]\n","    else:\n","        left_indices = get_node_indices(node.left)\n","        right_indices = get_node_indices(node.right)\n","        return left_indices + right_indices\n","\n","def find_node_path(layer, node_id, root):\n","    \"\"\"\n","    Finds the path from root node to the node with given node_id.\n","    Returns a list of choices ('left' or 'right') to traverse the path.\n","    \"\"\"\n","    def traverse(node, path=''):\n","        if node is None:\n","            return None\n","        if node.id == node_id:\n","            return path\n","        left_path = traverse(node.left, path + 'L')\n","        right_path = traverse(node.right, path + 'R')\n","        if left_path:\n","            return left_path\n","        if right_path:\n","            return right_path\n","        return None\n","\n","    return traverse(root)\n","\n","def get_cluster_by_path(path, root):\n","    \"\"\"\n","    Navigates the hierarchical clustering tree from the root node\n","    based on the given sequence of 'left' and 'right' choices.\n","    Returns the cluster node reached after following the path.\n","    \"\"\"\n","    node = root\n","    for direction in path:\n","        if direction == 'L':\n","            node = node.left\n","        elif direction == 'R':\n","            node = node.right\n","        else:\n","            raise ValueError(\"Invalid direction: {}\".format(direction))\n","    return node\n","\n","def get_neuronpedia_quick_list(\n","    features: list[int],\n","    layer: int,\n","    model: str = \"gpt2-small\",\n","    dataset: str = \"res-jb\",\n","    name: str = \"temporary_list\",\n","    setting: str = \"average\",\n","):\n","    url = \"https://neuronpedia.org/quick-list/\"\n","    name = urllib.parse.quote(name)\n","    url = url + \"?name=\" + name\n","    list_feature = [\n","        {\"modelId\": model, \"layer\": f\"{layer}-{dataset}\", \"index\": str(feature)}\n","        for feature in features\n","    ]\n","    url = url + \"&features=\" + urllib.parse.quote(json.dumps(list_feature))\n","    print(url)\n","    return url\n","\n","def build_cluster(layer, feature_id, height=3, setting = 'average', verbose=True):\n","    layer = int(layer)\n","    root = roots[setting][layer]\n","    node_path = find_node_path(layer, feature_id, root)\n","    cluster_path = node_path[:-height]\n","    cluster = get_cluster_by_path(cluster_path, root=root)\n","    indices = get_node_indices(cluster)\n","    list_name = f'height {height} above L{layer}f{feature_id} with cluster setting: {setting}'\n","    url = get_neuronpedia_quick_list(indices, layer, name=cluster_path)\n","    if verbose:\n","        print(f'path to node: {node_path}')\n","        print(f'path to cluster: {cluster_path}')\n","        print(f'features in cluster: {indices}')\n","        return indices"],"metadata":{"id":"PQ9XAy7eY82n","executionInfo":{"status":"aborted","timestamp":1720210304457,"user_tz":240,"elapsed":13,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helper methods related to the contrastive pair\n","import random\n","import json\n","\n","def create_contrastive_pairs(dataset, num_pairs=100):\n","    contrastive_pairs = []\n","\n","    for data in dataset:\n","        question = data[\"question\"]\n","        statement = data[\"statement\"]\n","        answer_matching = data[\"answer_matching_behavior\"].strip()\n","        answer_not_matching = data[\"answer_not_matching_behavior\"].strip()\n","\n","        # Create positive example\n","        positive_prompt = f\"{question}\\n{statement}\\nA) {answer_matching}\\nB) {answer_not_matching}\\nAnswer:\"\n","        positive_completion = \" A\"\n","\n","        # Create negative example\n","        negative_prompt = f\"{question}\\n{statement}\\nA) {answer_matching}\\nB) {answer_not_matching}\\nAnswer:\"\n","        negative_completion = \" B\"\n","\n","        contrastive_pairs.append({\n","            \"positive_prompt\": positive_prompt,\n","            \"positive_completion\": positive_completion,\n","            \"negative_prompt\": negative_prompt,\n","            \"negative_completion\": negative_completion\n","        })\n","\n","    # Ensure we have at least the requested number of pairs\n","    while len(contrastive_pairs) < num_pairs:\n","        contrastive_pairs.extend(contrastive_pairs)\n","\n","    # Randomly select the requested number of pairs\n","    return random.sample(contrastive_pairs, num_pairs)\n"],"metadata":{"id":"eYdgrd4tSZ3h","executionInfo":{"status":"aborted","timestamp":1720210304458,"user_tz":240,"elapsed":14,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specify dataset path\n","dataset_path = ''\n","\n","with open(dataset_path, 'r') as f:\n","    dataset = [json.loads(line) for line in f]\n","\n","# Create contrastive pairs\n","contrastive_pairs = create_contrastive_pairs(dataset)\n","\n","# Print a sample pair\n","print(\"Sample contrastive pair:\")\n","print(\"Positive prompt:\", contrastive_pairs[0][\"positive_prompt\"])\n","print(\"Positive completion:\", contrastive_pairs[0][\"positive_completion\"])\n","print(\"\\nNegative prompt:\", contrastive_pairs[0][\"negative_prompt\"])\n","print(\"Negative completion:\", contrastive_pairs[0][\"negative_completion\"])"],"metadata":{"id":"II1aZei0U7jO","executionInfo":{"status":"aborted","timestamp":1720210304458,"user_tz":240,"elapsed":14,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title #Usage\n","#@markdown Pick any layer, any feature from Joseph Bloom's GPT-2-small SAEs on the residual stream. Valid feature ids are between 0 and 24575.\n","\n","#@markdown `build_cluster` will return a list of features related to it, and a neuronpedia link to visualize of all of them.\n","\n","#@markdown If you use your own linkages for a different model, the features will still be related but the neuronpedia data won't be valid!\n","\n","#@markdown The `height` parameter controls how large the cluster is, by including more distant features.\n","\n","#@markdown If `height` is 6 or more, the URL might be too long to function.\n","\n","layer = str(LAYER) #@param [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","feature_id = 8301 #@param {type: \"integer\"}\n","height = 2 #@param {type: \"slider\", min:1, max:8}\n","setting = 'average' #@param ['average', 'complete', 'weighted']\n","indices = build_cluster(\n","    layer=layer,\n","    feature_id=feature_id,\n","    height=height,\n","    setting=setting,\n",")"],"metadata":{"id":"xd9RJqKhoz6y","executionInfo":{"status":"aborted","timestamp":1720210304458,"user_tz":240,"elapsed":13,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(indices)"],"metadata":{"id":"TBVCQfEpLsw2","executionInfo":{"status":"aborted","timestamp":1720210304459,"user_tz":240,"elapsed":13,"user":{"displayName":"sinem erisken","userId":"10098141094175015247"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we have the clusters for a specific task, now we'll try to use the eval to steer the model in an interesting way\n","\n","```\n","# This is formatted as code\n","```\n","\n"],"metadata":{"id":"CpGC3iiMRMVk"}},{"cell_type":"markdown","source":[],"metadata":{"id":"2PjRaRANWC0g"}}]}